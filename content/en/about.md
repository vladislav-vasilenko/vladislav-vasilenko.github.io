Experienced ML/DL engineer focused on generative models: Diffusion, Flow-Matching, DiT. Worked with SDXL and Flux architectures, applied LoRA/QLoRA for adaptation, experimented with ControlNet and IP-Adapter.

Build reproducible pipelines (Hydra, MLflow, W&B), optimize inference (xFormers, Flash-Attention, scheduler tuning) and prepare models for production (Docker, CI/CD, Triton, ONNX).

Actively experimenting with LLMs hands-on: built autograd engine and GPT-style Transformer decoder from scratch, studied Multi-Head Attention internals, gradient stability, and ablation analysis. Currently deep-diving into LLM compression — Quantization (INT8, AWQ, SmoothQuant), LoRA, Pruning (structured & N:M sparsity), Knowledge Distillation, and KV-Cache optimization — progressing from nanoGPT to Llama 3 (1B) scale.

Proficient in **Python (Expert)** and **PyTorch (Advanced)**, with a secondary focus on TensorFlow and JAX. Extensive experience in LLM orchestration, LangChain, and cloud infrastructure (SageMaker, GCP, Paperspace). Senior-level competence in Docker, MLOps, and industrial software engineering.
