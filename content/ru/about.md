Опытный ML/DL инженер с фокусом на генеративные модели: Diffusion, Flow-Matching, DiT. Работал с архитектурами SDXL и Flux, применял LoRA/QLoRA для адаптации, экспериментировал с ControlNet и IP-Adapter.

Собираю воспроизводимые пайплайны (Hydra, MLflow, W&B), оптимизирую инференс (xFormers, Flash-Attention, тюнинг шедулеров) и готовлю модели к продакшену (Docker, CI/CD, Triton, ONNX).

Активно экспериментирую с LLM на практике: собрал autograd-движок и GPT-декодер с нуля, изучил внутреннее устройство Multi-Head Attention, стабильность градиентов и абляционный анализ. Сейчас глубоко погружён в сжатие LLM — Квантизация (INT8, AWQ, SmoothQuant), LoRA, Прунинг (структурный и N:M sparsity), Дистилляция знаний и оптимизация KV-Cache — от nanoGPT до масштаба Llama 3 (1B).

Эксперт в **Python** и продвинутый уровень в **PyTorch (Advanced)**, также работаю с TensorFlow и JAX. Глубокий опыт в оркестрации LLM, LangChain и облачной инфраструктуре (SageMaker, GCP, Paperspace). Уровень Senior в Docker, MLOps и промышленной разработке ПО.
